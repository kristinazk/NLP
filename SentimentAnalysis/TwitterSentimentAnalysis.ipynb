{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "933cbb23-1d82-4736-add3-f56fa5bc8365",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis\n",
    "\n",
    "**Author : Kristina Zakaryan**\n",
    "\n",
    "Steps taken to complete project: \n",
    "- Data Preprocessing (Tokenization, Stop-words Removal, Stemming, TF-IDF Vectorization) \n",
    "- Model Training \n",
    "- Interpretation of Results and Possible Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b738b1-9262-45cf-ad39-f93730a4e77e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a9968b-b1b9-4aaa-b1e7-8be089517fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbdf6b76-73c6-4454-9baf-b188fbc1590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the neccessary imports in one place\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae310199-3b35-43e2-9320-3d56e5219789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/kazanova/sentiment140\n"
     ]
    }
   ],
   "source": [
    "# downloading the dataset from kaggle\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "PATH = \"kazanova/sentiment140\"\n",
    "\n",
    "api.dataset_download_files(PATH, path=\"data/\", unzip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fc998c-a375-47b1-a73d-3a684f34471b",
   "metadata": {},
   "source": [
    "Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b973176a-9af7-47fe-a09b-cb4243bae3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/tina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/tina/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt') # needed for word_tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "porter_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4725c481-0025-43d6-b615-eadd4e5866df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c6be0e-2294-4879-ad91-69bed82bf5ea",
   "metadata": {},
   "source": [
    "Since column names were not initially included in the dataset, we add them manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "518b10ff-90df-4594-adf9-4a6840f28c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "\n",
    "data = pd.read_csv('data/training.1600000.processed.noemoticon.csv', encoding=\"ISO-8859-1\", header=None, names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4e69ac4-0f7c-40a0-aca4-eba82c3524f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b76f36-a152-44e1-8635-b3979ef5435b",
   "metadata": {},
   "source": [
    "The dataset contains 2 labels: **0** - negative, **4** - positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac233718-3691-43d1-8c1f-36d897a4265d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 4}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a774550f-4fef-4bf6-8437-e5cbe5a4ec19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "689b286a-9741-46fa-b851-631b9de015cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target    0\n",
       "ids       0\n",
       "date      0\n",
       "flag      0\n",
       "user      0\n",
       "text      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1adffeb-0e73-47cb-8126-551d87cb158b",
   "metadata": {},
   "source": [
    "There are no missing values in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b42b81-c829-4da3-899d-de0b88ba674d",
   "metadata": {},
   "source": [
    "Now let's understand the distribution of the target variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "486807b0-9e05-4ca4-9e19-b87a9ce66a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    800000\n",
       "4    800000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8c6ccd-2501-4238-afc3-37c33482c7c4",
   "metadata": {},
   "source": [
    "The dataset is perfectly balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9348c7a6-d10b-4f46-9608-e0ddebdca48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace({'target' : {4:1}}, inplace=True) # makes the target variable labels more intuitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3529841-7b17-4f78-a03f-fdad08c7336c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(data['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65d3444-fee4-4c34-92e9-c3f37e1726a5",
   "metadata": {},
   "source": [
    "The majority of the columns have 0 importance in our task, so we can remove them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5297a52-e148-46ca-9b14-811eea5cc0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([\"ids\", \"date\", \"flag\", \"user\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617d7884-95cd-4823-ad6d-52ecb973d497",
   "metadata": {},
   "source": [
    "Now it is time to work on the textual part of our dataset, let's first remove some insignificant words via regular expressions. Here are some considerations:\n",
    "1) After analyzing the text in the tweets, we can notice a huge number of mentions (starting with @) and links that have 0 significance for our model and thus need to be removed.\n",
    "2) Numbers will be removed too, since in 1 context a number can have a positive connotation, but in another it can be perceived as negative so they only add ambiguity to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e7681fd-3687-4102-99d9-cfe3f079c856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_preprocessor(tweet):\n",
    "    # remove @mentions\n",
    "    text = re.sub(r'@\\w+', '', tweet)\n",
    "    \n",
    "    # remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "\n",
    "    # remove symbols, digits, etc..\n",
    "    text = re.sub('[^a-z\\s]', ' ', text)\n",
    "\n",
    "    #remove multiple spaces\n",
    "    return re.sub(r'\\s+', ' ', text).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca2061c-9411-4318-a8b8-6c84a97bd606",
   "metadata": {},
   "source": [
    "Another important step in our NLP pipeline is **Tokenization**. This is the stage where we break our text down to smaller chunks, in order to convert raw text into individual units (tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c016e71b-9d2b-493e-a0fd-8d2d63b74be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(tweet):\n",
    "    return word_tokenize(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c7dfb8-aad3-4302-a9e7-7d9cf5365568",
   "metadata": {},
   "source": [
    "Stopwords do not add any meaning to our actual data, so we choose to remove these words from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5f2ce99-2740-45a7-bd56-9d4c3c7da7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_words_remover(tokenized_tweet):\n",
    "    return [word for word in tokenized_tweet if word not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97651b37-71de-44b0-badf-d8da66e072d2",
   "metadata": {},
   "source": [
    "A significant way to preprocess test data is **Stemming**: reducing the word to its root. This way our dataset becomes smaller, without\n",
    "having to lose significant amount of information contained in it.\n",
    "\n",
    "Another way to perform the latter would be **Lemmatization** but it is a heavier operation and since afterwards we will transform the tweets via TF-IDF vectorization, Stemming was chosen as a preprocessing step, since this method does not really care about the content being 'comprehensible'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3013ae60-a1c0-46a9-8825-8fab3107e931",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "def stemmer(word):\n",
    "    return porter_stemmer.stem(word)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb198c8-19d2-4178-ab13-590793765dde",
   "metadata": {},
   "source": [
    "Below is the full preprocessing pipeline that gathers all the methods defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bf76ed4-c5ab-4191-b779-d9546b5e7117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_pipeline(tweet):\n",
    "    tweet_lower = tweet.lower()\n",
    "    tweet_regex = regex_preprocessor(tweet_lower)\n",
    "    tweet_array_tok = tokenizer(tweet_regex)\n",
    "    tweet_wo_sw = stop_words_remover(tweet_array_tok)\n",
    "    tweet_stemmed = [stemmer(word) for word in tweet_wo_sw]\n",
    "    return ' '.join(tweet_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "570c35ff-5d28-4a46-b4a1-b2763e1fc40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_preprocessed'] = data['text'].apply(preprocessing_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b4cd77c-3d0b-48cc-ba74-7f917c6cff00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              bummer shoulda got david carr third day\n",
       "1    upset updat facebook text might cri result sch...\n",
       "2         dive mani time ball manag save rest go bound\n",
       "3                      whole bodi feel itchi like fire\n",
       "4                                        behav mad see\n",
       "Name: text_preprocessed, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text_preprocessed'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67763a28-00f8-49b3-ab60-09672c00757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['text_preprocessed'].values\n",
    "y = data['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec847d57-242a-421b-94b2-0a10a097d6f6",
   "metadata": {},
   "source": [
    "In order to feed the values into the model we will use the train_test_split method provided by scikit-learn. It is also important to shuffle the dataset to avoid bias based on the order of particular data points. To preserve the balance between labels, we set **stratify=Y**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5136626-8229-42ff-8401-2a4990b03ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da50c90-3acb-40cb-9d62-606bb9e3dac0",
   "metadata": {},
   "source": [
    "Now it is time to turn our text into digital vectors. For that we will use the TF-IDF vectorizer, which prioritizes rare words between documents, thus minimizint the impact of very common words that appear in all the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9fed3f6-2e6b-4f66-814d-2b1f3a6575b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "383e1aaf-d2ce-4290-b525-aec510d88663",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39413dac-5883-45b0-8a45-a602542481e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200000, 182523)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a031a8a-6b67-4aca-bf66-c1900d778a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 182523)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c0cd433-5afb-48a6-aec4-c9dfa9a38d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_accuracy_conf_mat(y_real, y_pred):\n",
    "    print('Model test Accuracy: ', accuracy_score(y_real, y_pred))\n",
    "    print('Confusion Matrix', confusion_matrix(y_real, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914637c2-94f9-4d93-8fd6-548369dda969",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Model Selection \n",
    "\n",
    "In this section we will train a Machine Learning Classifier to predict if the tweet is negative or positive.\n",
    "\n",
    "Models that have been considered for the task:\n",
    "-  Logistic Regression\n",
    "-  Naive Bayes (Multinomial)\n",
    "\n",
    "Since our dataset is huge, models like KNN, SVM, Random Forest are too heavy to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "622bbfb8-2a55-48fa-8059-1c4840bab668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model test Accuracy:  0.77243\n",
      "Confusion Matrix [[150252  49359]\n",
      " [ 41669 158720]]\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(max_iter=10000)\n",
    "\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lreg = log_reg.predict(X_test)\n",
    "\n",
    "display_accuracy_conf_mat(y_test, y_pred_lreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d019895-9377-401a-93a3-431a2c2b4b6f",
   "metadata": {},
   "source": [
    "It is quite common to use Gaussian Naive Bayes, but since we are working with text on which we have calculated frequencies, it makes sense to use a Multinomial Naive Bayes, which assumes that features are counts (discrete events) that follow a multinomial distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a95eaee-3894-48d4-a0d4-f487399fd73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model test Accuracy:  0.755465\n",
      "Confusion Matrix [[153187  46424]\n",
      " [ 51390 148999]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred_nb = nb.predict(X_test)\n",
    "\n",
    "display_accuracy_conf_mat(y_test, y_pred_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089657c4-620f-42ef-929f-ecd5c62e2226",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Interpretation of Results\n",
    "\n",
    "We have obtained an accuracy of > 75% which is not bad for the following dataset. If we want to reach a higher accuracy, the next step to consider would be training a neurtal network by carefully choosing the parameters.\n",
    "\n",
    "But standard dense feedforward NNs on TF-IDF rarely outperform Logistic Regression by more than 1â€“2% unless you carefully tune embeddings, layers, and regularization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
