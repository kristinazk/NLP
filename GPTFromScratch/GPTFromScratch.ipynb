{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fcb3f56-4b9a-484d-929a-7720957264ed",
   "metadata": {},
   "source": [
    "# GPT From Scratch\n",
    "\n",
    "Based on theory from the paper titled ['Attention is all you need'](https://arxiv.org/pdf/1706.03762) & a Youtube video named [\n",
    "Let's build GPT: from scratch, in code, spelled out](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=1221s) by Andrej Karpathy.\n",
    "\n",
    "The **dataset** used in this project : [Tiny Shakespeare](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt) (the totality of all the words written by Shakespeare).\n",
    "\n",
    "The **goal** of this model is predicting the next token (subword in our case) based on the given context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3e6d74f3-ef9a-4898-a7e1-8723705dac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the necessary imports in one place\n",
    "import wget\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2dc981fc-0879-4ef5-a7dd-b4aa2f20cd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [......................................................] 1115394 / 1115394"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'input (2).txt'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "wget.download(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a3c7ab-fc5b-4bbc-bb45-712f8055edf9",
   "metadata": {},
   "source": [
    "## Importing and Preprocessing necessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "017986bb-e3e7-4051-bac4-9b3894f6366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2a23ef41-7f47-4b46-a9e2-52ce53e38f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us\n"
     ]
    }
   ],
   "source": [
    "print(text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3c84bfe4-dc34-418d-89e2-d95821b2d86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset in characters: 1115394\n"
     ]
    }
   ],
   "source": [
    "print('Length of dataset in characters:', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb04617-c91c-48ba-a988-6d4f56dda621",
   "metadata": {},
   "source": [
    "Let's obtain all the unique characters in our dataset (sorted) <br>\n",
    "(when printing the ' 's were placed to see that the first element is a blank space character, which is also considered a character in the vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cc0754-f903-4158-8827-5ca03bff30af",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "To keep things simple we are using a character-level tokenizer. But real GPTs mainly use more advanced tokenizers such as BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f37757e1-dced-4c76-b7d6-96f042a7364e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the characters present in the dataset:  \n",
      "' ' ' '!' '$' '&' ''' ',' '-' '.' '3' ':' ';' '?' 'A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W' 'X' 'Y' 'Z' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o' 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z\n",
      "Vocabulary Length :  65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print('All the characters present in the dataset: ', \"' '\".join(chars))\n",
    "print('Vocabulary Length : ', len(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec33556d-dff2-410b-9ae8-426524abc404",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "\n",
    "In this section we find a way to turn our string into an integer representation, and we define 2 crucial components of the GPT:\n",
    "- Encoder : takes a string and outputs a list of integers\n",
    "- Decoder : takes a list of integers and outputs corresponding string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5d6f553e-ba01-4575-a126-fa1f1e4430b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encoder = lambda seq: [stoi[char] for char in seq]\n",
    "decoder = lambda int_list: ''.join([itos[i] for i in int_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e8dfa8c1-1b15-4727-92d8-cb8f0b0308f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 1, 58, 46, 43, 56, 43]\n",
      "hello there\n"
     ]
    }
   ],
   "source": [
    "print(encoder('hello there'))\n",
    "print(decoder([46, 43, 50, 50, 53, 1, 58, 46, 43, 56, 43]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c658417-79b3-428e-bd6f-3521aadfe496",
   "metadata": {},
   "source": [
    "Lets apply this to the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "98c3306a-4515-420d-aa70-c1c23910480f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encoder(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0acc41-6a14-4644-8147-303e31f78d55",
   "metadata": {},
   "source": [
    "Let's split the data into train and validation sets:\n",
    "- 90% train\n",
    "- 10% validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "be274784-e833-4b4c-98d0-44f1968e1f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8ce8f3-9277-4317-a1c8-a9b210454e53",
   "metadata": {},
   "source": [
    "Key point: we are not going to feed all the data to a transformer at one, since it would be too computationally expensive. We sample random chunks of data that have a max length (block_size) and work with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d8bc7c13-ae3c-4b1d-8699-36964611b7ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb80d98e-4bc7-4117-9830-33096ee178b9",
   "metadata": {},
   "source": [
    "Given a block_size the model will have to make 8 predictions, so in our example: <br>\n",
    "given 18, predict 47, <br>\n",
    "given 18, 47, predict 56 etc. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a7978282-695a-43c0-acea-bc26ff970bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([18]) the target: 47\n",
      "When input is tensor([18, 47]) the target: 56\n",
      "When input is tensor([18, 47, 56]) the target: 57\n",
      "When input is tensor([18, 47, 56, 57]) the target: 58\n",
      "When input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "When input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1]\n",
    "\n",
    "for t in range(block_size): # t for time dimension\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'When input is {context} the target: {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe4f69e-b0bb-4d90-8beb-ed685d01758a",
   "metadata": {},
   "source": [
    "The subtility of creating the context in this way is making the transformer see not only the last word and the target, but also the words that came before it, forming a context.<br><br>\n",
    "We will also have batches of certain dimensions containing multiple chunks of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cd8bd3b5-6acf-459c-9f95-dc0b2b0f69c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "BATCH_SIZE = 4\n",
    "BLOCK_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "067ef498-763d-42c4-bac0-9272394e5de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets: torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split):\n",
    "    # generate a batch of data of inputs x and target y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE, ))\n",
    "    x = torch.stack([data[i:i + BLOCK_SIZE] for i in ix])\n",
    "    y = torch.stack([data[i + 1:i + BLOCK_SIZE + 1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:', xb.shape)\n",
    "print(xb)\n",
    "print('targets:', yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ccb16172-9579-44f5-b7f2-e4d4a3b335f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is [24] the target is 43\n",
      "when input is [24, 43] the target is 58\n",
      "when input is [24, 43, 58] the target is 5\n",
      "when input is [24, 43, 58, 5] the target is 57\n",
      "when input is [24, 43, 58, 5, 57] the target is 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target is 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target is 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target is 39\n",
      "when input is [44] the target is 53\n",
      "when input is [44, 53] the target is 56\n",
      "when input is [44, 53, 56] the target is 1\n",
      "when input is [44, 53, 56, 1] the target is 58\n",
      "when input is [44, 53, 56, 1, 58] the target is 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target is 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target is 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target is 1\n",
      "when input is [52] the target is 58\n",
      "when input is [52, 58] the target is 1\n",
      "when input is [52, 58, 1] the target is 58\n",
      "when input is [52, 58, 1, 58] the target is 46\n",
      "when input is [52, 58, 1, 58, 46] the target is 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target is 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target is 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target is 46\n",
      "when input is [25] the target is 17\n",
      "when input is [25, 17] the target is 27\n",
      "when input is [25, 17, 27] the target is 10\n",
      "when input is [25, 17, 27, 10] the target is 0\n",
      "when input is [25, 17, 27, 10, 0] the target is 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target is 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target is 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target is 39\n"
     ]
    }
   ],
   "source": [
    "for b in range(BATCH_SIZE):\n",
    "    for t in range(BLOCK_SIZE):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {context.tolist()} the target is {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d239d83-0328-4b22-980e-a4673d343e11",
   "metadata": {},
   "source": [
    "Before creating the GPT, we will first try the dataset on lighter models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df196f29-a216-4ef5-81af-cdc44fc3dd62",
   "metadata": {},
   "source": [
    "#### Bigram Model\n",
    "\n",
    "We'll start with the Bigram model, which predicts the next word based on the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "db94dc42-bb2e-4bdc-8d5d-0d92e9b3aead",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token reads off the logits for the next token directly from the lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B, T) tensors of integers\n",
    "        logits = self.token_embedding_table(idx) # scores for next character in sequence of size (B, T, C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # modifications of shape since cross_entropy expects to see another shape\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C) \n",
    "            targets = targets.view(B * T)\n",
    "            \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(idx)\n",
    "            logits = logits[:, -1, :] # make it (B, C)\n",
    "            probs = F.softmax(logits, dim=-1) # applying softmax to get probas\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # shape (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "            \n",
    "        return idx  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fdaa4ca7-b3f8-41bc-b189-c9935a15f7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.5330, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Jx:E$;\n",
      "'ana:.p&mY&3Ww3GRMCyRGmD$SLjtLj AZwuoF3WJYQyqxIAS-Mu&RHeaTwWPfiS\n",
      "gti3dcpNd be:R;XvyNe3YChSL!Z\n"
     ]
    }
   ],
   "source": [
    "bigram_model = BigramLanguageModel(vocab_size)\n",
    "logits, loss = bigram_model(xb, yb)\n",
    "\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decoder(bigram_model.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bed987f-8a11-47d4-b6a3-c4098e3fa83b",
   "metadata": {},
   "source": [
    "As we can see the predictions are far from being something even related to text we can understand, let alone Shakespeare. Let's add an optimizer first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "84505d35-3236-4e75-8734-4419c8a18756",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(bigram_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c9f83e-fab0-4d0d-badf-31172bc7e547",
   "metadata": {},
   "source": [
    "Next we will see a typical training setup of 100 iterations on PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "54d50cd7-17cd-4396-91cf-f9a3a39917f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4513444900512695\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 # let's pick a bigger batch size\n",
    "\n",
    "for steps in range(10000):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = bigram_model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b10e21ca-db53-442a-8b8d-d745a50f264c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Boemanse oue bethefin,\n",
      "HNept,\n",
      "\n",
      "CIZx-FRoke t;\n",
      "INI h-e the w'f ce?nthtur\n",
      "This ianspy GOMan y s g.\n",
      "R s Dowouth r areny mern met\n",
      "I is f hait heard LI has ourjNy wes:\n",
      "aimede in whe wndeildr mad:\n",
      "\n",
      "Aly BEdy\n",
      "Ord is RY: tof ath bast:\n",
      "\n",
      "\n",
      "Taif t amy co.\n",
      "FRI ans; tanme:\n",
      "NCIXRI my the ts.\n",
      "PEY ce hecive If T:\n",
      "Itit\n"
     ]
    }
   ],
   "source": [
    "print(decoder(bigram_model.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda57376-911d-4709-ac66-8ed25331d02f",
   "metadata": {},
   "source": [
    "As we can see, the model has gone through some improvement. However, we can clearly see that we are far from being able to generate Shakespeare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eb79ae-7def-47cb-8deb-ebc869222636",
   "metadata": {},
   "source": [
    "Up until this point, we have never used the **context** of the text to make predictions, since the model was too simple. It is important not to look at the 'future' words, so we only consider the (average) context that comes previously:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7649cbe-b57c-4af1-8569-f318b9ef8cfc",
   "metadata": {},
   "source": [
    "## The Mathematical Trick in Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e7a7da-0dbf-4625-92f9-109b824f80fe",
   "metadata": {},
   "source": [
    "Let's look at a toy example :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5b73fd37-3993-4bf8-a5fe-adf419c67d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad57d816-6207-4dc4-b4de-037aba88209b",
   "metadata": {},
   "source": [
    "### Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "632ef66c-e0f4-4e2b-b1d7-3ef715da93fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bow = torch.zeros((B, T, C))\n",
    "\n",
    "for batch in range(B):\n",
    "    for t in range(T):\n",
    "        x_prev = x[batch, :t+1] # (t, C)\n",
    "        x_bow[batch, t] = torch.mean(x_prev, 0) # averaging out the time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2274455c-eae5-4d20-b3d1-cbc00246ab42",
   "metadata": {},
   "source": [
    "This is how we get an average of the context that came before. However, there is a better solution by multiplying normalized matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c884c1a6-f09b-4612-a6b7-17e27a98a7ee",
   "metadata": {},
   "source": [
    "### Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "83b05e46-0d40-4a9d-9fc5-bf08b3c640cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T, T)) # tril() creates a lower triangular matrix\n",
    "wei = wei / wei.sum(1, keepdim=True) # normalizing the matrix to get the average after matrix mult\n",
    "x_bow2 = wei @ x # (B, T, T) @ (B, T, C) --> (B, T, C)\n",
    "torch.allclose(x_bow, x_bow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b8c3a7-93da-4d81-b040-03cb5e1bb6cb",
   "metadata": {},
   "source": [
    "Another version consists in using SoftMax to get the right matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6d9056-bbda-4808-ae14-d7ab232f3f5b",
   "metadata": {},
   "source": [
    " ### Version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "4a00d122-451d-4ea1-bb54-ea46bb45a1e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "292ebfe8-b2b5-4348-8398-b5764c0333fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # if tril == 0 replace with -inf\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c3de9b9b-6697-44c5-8875-2468d8f892e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = F.softmax(wei, dim=-1)\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "bbd31815-4f86-4ffb-a37a-fe2ef8e9b326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_bow3 = wei @ x\n",
    "torch.allclose(x_bow, x_bow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf9c80c-72b8-4482-a797-9dde0e5c42f1",
   "metadata": {},
   "source": [
    "So the takeaway from this part of the notebook is that we can do weighted aggregations of the past elements by doing matrix multiplications on a lower triangular matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0af8d9-e49a-4c8c-b042-a8ffc8c1ee0f",
   "metadata": {},
   "source": [
    "## Self-Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7d6257-2172-45fd-82f1-ae9f9ab1602f",
   "metadata": {},
   "source": [
    "Each token in the data has 2 representations : Query and Key.\n",
    "We calculate the dot product of each Query word with Key words. If the words are 'similar', we will get a high score and we will get to learn more about that specific token as opposed to any other token in the sequence.\n",
    "<br>\n",
    "In most real-world models multiple heads are used (independent computations of attention) so that we are able to capture **different relationships in different heads**.\n",
    "\n",
    "The value is the agregated result.\n",
    "\n",
    "The self-attention mechanism is computed as:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\Big(\\frac{Q K^T}{\\sqrt{d_k}}\\Big) V\n",
    "$$\n",
    "\n",
    "the sqrt(dk) (where dk is head_size) is an important normalization step to preserve the initial variance. Since wei will then be fed into a Softmax function, it is important for wei to be **fairly diffused**.\n",
    "\n",
    "If wei takes on very positive or very negative numbers inside it, Softmax might converge into one-hot vectors, and that is not what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "861f2608-a6ad-4669-bc63-05da24acc249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "head_size = 16 # hyperparameter\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x) # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei = q @ k.transpose(-2, -1) * head_size ** -0.5 # similarity matrix\n",
    "# (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "\n",
    "# wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1) # last dimension\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "79e4f5e7-1610-4ab1-8190-d40c3dc5a870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3966, 0.6034, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3069, 0.2892, 0.4039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3233, 0.2175, 0.2443, 0.2149, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1479, 0.2034, 0.1663, 0.1455, 0.3369, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1259, 0.2490, 0.1324, 0.1062, 0.3141, 0.0724, 0.0000, 0.0000],\n",
       "        [0.1598, 0.1990, 0.1140, 0.1125, 0.1418, 0.1669, 0.1061, 0.0000],\n",
       "        [0.0845, 0.1197, 0.1078, 0.1537, 0.1086, 0.1146, 0.1558, 0.1553]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3544333a-8569-487b-b91b-483658f26701",
   "metadata": {},
   "source": [
    "So in this example (last row) we can see that the element in the 3rd position was interesing to the last element since it has the highest weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528e994b-b8f1-4997-8abd-ea5ea806c00b",
   "metadata": {},
   "source": [
    "### Some Useful Notes on Attention:\n",
    "- Attention is a **communication mechanism**.\n",
    "- There is no **notion of space**. That is why we have to positionally encode tokens to make use of this information.\n",
    "- Each example across batch dimension is processed **completely independently**.\n",
    "- In some use cases it is ok to look at the words that come in the future, e.g. Sentiment Analysis. It is then called an Encoder block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ccae87-b75b-466c-a4a6-8dfcf3efabed",
   "metadata": {},
   "source": [
    "Full trainable models (bigram.py and gpt.py) can be found in the GitHub repository this Notebook is Uploaded on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT From Scratch",
   "language": "python",
   "name": "gptfromscratch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
