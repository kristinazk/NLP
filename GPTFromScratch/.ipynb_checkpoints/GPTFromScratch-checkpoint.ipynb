{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fcb3f56-4b9a-484d-929a-7720957264ed",
   "metadata": {},
   "source": [
    "# GPT From Scratch\n",
    "\n",
    "Based on theory from the paper titled ['Attention is all you need'](https://arxiv.org/pdf/1706.03762).\n",
    "\n",
    "The **dataset** used in this project : [Tiny Shakespeare](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt) (the totality of all the words written by Shakespeare).\n",
    "\n",
    "The **goal** of this model is predicting the next token (subword in our case) based on the given context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3e6d74f3-ef9a-4898-a7e1-8723705dac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the necessary imports in one place\n",
    "import wget\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dc981fc-0879-4ef5-a7dd-b4aa2f20cd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [......................................................] 1115394 / 1115394"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'input (1).txt'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "wget.download(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a3c7ab-fc5b-4bbc-bb45-712f8055edf9",
   "metadata": {},
   "source": [
    "## Importing and Preprocessing necessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "017986bb-e3e7-4051-bac4-9b3894f6366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a23ef41-7f47-4b46-a9e2-52ce53e38f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us\n"
     ]
    }
   ],
   "source": [
    "print(text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c84bfe4-dc34-418d-89e2-d95821b2d86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset in characters: 1115394\n"
     ]
    }
   ],
   "source": [
    "print('Length of dataset in characters:', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb04617-c91c-48ba-a988-6d4f56dda621",
   "metadata": {},
   "source": [
    "Let's obtain all the unique characters in our dataset (sorted) <br>\n",
    "(when printing the ' 's were placed to see that the first element is a blank space character, which is also considered a character in the vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cc0754-f903-4158-8827-5ca03bff30af",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "To keep things simple we are using a character-level tokenizer. But real GPTs mainly use more advanced tokenizers such as BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f37757e1-dced-4c76-b7d6-96f042a7364e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the characters present in the dataset:  \n",
      "' ' ' '!' '$' '&' ''' ',' '-' '.' '3' ':' ';' '?' 'A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W' 'X' 'Y' 'Z' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o' 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z\n",
      "Vocabulary Length :  65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print('All the characters present in the dataset: ', \"' '\".join(chars))\n",
    "print('Vocabulary Length : ', len(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec33556d-dff2-410b-9ae8-426524abc404",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "\n",
    "In this section we find a way to turn our string into an integer representation, and we define 2 crucial components of the GPT:\n",
    "- Encoder : takes a string and outputs a list of integers\n",
    "- Decoder : takes a list of integers and outputs corresponding string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d6f553e-ba01-4575-a126-fa1f1e4430b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encoder = lambda seq: [stoi[char] for char in seq]\n",
    "decoder = lambda int_list: ''.join([itos[i] for i in int_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8dfa8c1-1b15-4727-92d8-cb8f0b0308f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 1, 58, 46, 43, 56, 43]\n",
      "hello there\n"
     ]
    }
   ],
   "source": [
    "print(encoder('hello there'))\n",
    "print(decoder([46, 43, 50, 50, 53, 1, 58, 46, 43, 56, 43]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c658417-79b3-428e-bd6f-3521aadfe496",
   "metadata": {},
   "source": [
    "Lets apply this to the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98c3306a-4515-420d-aa70-c1c23910480f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encoder(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0acc41-6a14-4644-8147-303e31f78d55",
   "metadata": {},
   "source": [
    "Let's split the data into train and validation sets:\n",
    "- 90% train\n",
    "- 10% validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be274784-e833-4b4c-98d0-44f1968e1f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8ce8f3-9277-4317-a1c8-a9b210454e53",
   "metadata": {},
   "source": [
    "Key point: we are not going to feed all the data to a transformer at one, since it would be too computationally expensive. We sample random chunks of data that have a max length (block_size) and work with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8bc7c13-ae3c-4b1d-8699-36964611b7ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb80d98e-4bc7-4117-9830-33096ee178b9",
   "metadata": {},
   "source": [
    "Given a block_size the model will have to make 8 predictions, so in our example: <br>\n",
    "given 18, predict 47, <br>\n",
    "given 18, 47, predict 56 etc. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a7978282-695a-43c0-acea-bc26ff970bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([18]) the target: 47\n",
      "When input is tensor([18, 47]) the target: 56\n",
      "When input is tensor([18, 47, 56]) the target: 57\n",
      "When input is tensor([18, 47, 56, 57]) the target: 58\n",
      "When input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "When input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1]\n",
    "\n",
    "for t in range(block_size): # t for time dimension\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'When input is {context} the target: {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe4f69e-b0bb-4d90-8beb-ed685d01758a",
   "metadata": {},
   "source": [
    "The subtility of creating the context in this way is making the transformer see not only the last word and the target, but also the words that came before it, forming a context.<br><br>\n",
    "We will also have batches of certain dimensions containing multiple chunks of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cd8bd3b5-6acf-459c-9f95-dc0b2b0f69c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "BATCH_SIZE = 4\n",
    "BLOCK_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "067ef498-763d-42c4-bac0-9272394e5de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets: torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split):\n",
    "    # generate a batch of data of inputs x and target y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE, ))\n",
    "    x = torch.stack([data[i:i + BLOCK_SIZE] for i in ix])\n",
    "    y = torch.stack([data[i + 1:i + BLOCK_SIZE + 1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:', xb.shape)\n",
    "print(xb)\n",
    "print('targets:', yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ccb16172-9579-44f5-b7f2-e4d4a3b335f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is [24] the target is 43\n",
      "when input is [24, 43] the target is 58\n",
      "when input is [24, 43, 58] the target is 5\n",
      "when input is [24, 43, 58, 5] the target is 57\n",
      "when input is [24, 43, 58, 5, 57] the target is 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target is 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target is 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target is 39\n",
      "when input is [44] the target is 53\n",
      "when input is [44, 53] the target is 56\n",
      "when input is [44, 53, 56] the target is 1\n",
      "when input is [44, 53, 56, 1] the target is 58\n",
      "when input is [44, 53, 56, 1, 58] the target is 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target is 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target is 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target is 1\n",
      "when input is [52] the target is 58\n",
      "when input is [52, 58] the target is 1\n",
      "when input is [52, 58, 1] the target is 58\n",
      "when input is [52, 58, 1, 58] the target is 46\n",
      "when input is [52, 58, 1, 58, 46] the target is 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target is 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target is 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target is 46\n",
      "when input is [25] the target is 17\n",
      "when input is [25, 17] the target is 27\n",
      "when input is [25, 17, 27] the target is 10\n",
      "when input is [25, 17, 27, 10] the target is 0\n",
      "when input is [25, 17, 27, 10, 0] the target is 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target is 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target is 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target is 39\n"
     ]
    }
   ],
   "source": [
    "for b in range(BATCH_SIZE):\n",
    "    for t in range(BLOCK_SIZE):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {context.tolist()} the target is {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d239d83-0328-4b22-980e-a4673d343e11",
   "metadata": {},
   "source": [
    "Before creating the GPT, we will first try the dataset on lighter models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df196f29-a216-4ef5-81af-cdc44fc3dd62",
   "metadata": {},
   "source": [
    "#### Bigram Model\n",
    "\n",
    "We'll start with the Bigram model, which predicts the next word based on the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "db94dc42-bb2e-4bdc-8d5d-0d92e9b3aead",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token reads off the logits for the next token directly from the lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets):\n",
    "        # idx and targets are both (B, T) tensors of integers\n",
    "        logits = self.token_embedding_table(idx) # scores for next character in sequence of size (B, T, C)\n",
    "\n",
    "        # modifications of shape since cross_entropy expects to see another shape\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B * T, C) \n",
    "        targets = targets.view(B * T)\n",
    "        \n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(idx)\n",
    "            logits = logits[:, -1, :] # make it (B, C)\n",
    "            probs = F.softmax(logits, dim=1) # applying softmax to get probas\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # shape (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "            \n",
    "            return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fdaa4ca7-b3f8-41bc-b189-c9935a15f7f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m bigram_model \u001b[38;5;241m=\u001b[39m BigramLanguageModel(vocab_size)\n\u001b[0;32m----> 2\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m bigram_model(xb, yb)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(logits\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "bigram_model = BigramLanguageModel(vocab_size)\n",
    "logits, loss = bigram_model(xb, yb)\n",
    "\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06017b46-f47e-4de6-b4c7-38fa3a028180",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT From Scratch",
   "language": "python",
   "name": "gptfromscratch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
